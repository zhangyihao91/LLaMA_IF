{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9002295-b5dc-45ca-8b26-9a3084c93778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import math\n",
    "import hiq\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from llama import ModelArgs, Transformer, Tokenizer, LLaMA\n",
    "from llama.generation import sample_top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73f6a66b-f3ba-4767-8f7a-3834e8bfac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../7B/consolidated.00.pth', map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bb552a-21bf-44ee-b817-440dc7387402",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../7B/params.json', \"r\") as f:\n",
    "    params = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5649ba6b-0b0b-4deb-b5c7-1346a47d1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=512, max_batch_size=1, **params\n",
    ")\n",
    "tokenizer = Tokenizer('../tokenizer.model')\n",
    "model_args.vocab_size = tokenizer.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea5d0a5b-44f9-40ec-ba16-e0cbd52d5f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = Transformer(model_args)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "model.load_state_dict(checkpoint,strict= False)\n",
    "\n",
    "generator = LLaMA(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "161d2f5e-b4ac-46e1-917a-486544f084fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature: float = 0.8\n",
    "top_p: float = 0.95\n",
    "max_seq_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f149cb-21ee-4686-876c-ad71fbd7aeb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\"I believe the meaning of life is\"]\n",
    "\n",
    "max_gen_len = 256\n",
    "\n",
    "bsz = 1 \n",
    "params = params\n",
    "prompt_tokens = [generator.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "total_len = min(max_seq_len, max_gen_len + max_prompt_size)\n",
    "\n",
    "tokens = torch.full((bsz, total_len),generator.tokenizer.pad_id).cuda().long()\n",
    "\n",
    "for k, t in enumerate(prompt_tokens):\n",
    "    tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "input_text_mask = tokens != generator.tokenizer.pad_id\n",
    "start_pos = min_prompt_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729da221-3de2-49c5-9ee8-aa4e2840e07d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prev_pos = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for cur_pos in range(start_pos, total_len):\n",
    "        input_tensor = torch.cat((tokens[:, prev_pos:cur_pos],torch.tensor([[prev_pos]]).cuda()), 1)\n",
    "        if cur_pos == total_len-1:\n",
    "            break\n",
    "        logits = model(input_tensor)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "        next_token = next_token.reshape(-1)\n",
    "        # only replace token if prompt has already been generated\n",
    "        next_token = torch.where(\n",
    "            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "        )\n",
    "        tokens[:, cur_pos] = next_token\n",
    "        prev_pos = cur_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53dffc8-a427-44cd-a998-73df65f6fe98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.cat((tokens[:, prev_pos:cur_pos],torch.tensor([[prev_pos]]).cuda()), 1)\n",
    "\n",
    "token_len = input_tensor.shape[1] \n",
    "tokens = input_tensor[:, 0: token_len-1]\n",
    "start_pos = input_tensor[:, -1].item()\n",
    "_bsz, seqlen = tokens.shape\n",
    "h = model.tok_embeddings(tokens)\n",
    "model.freqs_cis = model.freqs_cis.to(h.device)\n",
    "freqs_cis = model.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "mask = None\n",
    "if seqlen > 1:\n",
    "    mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "    mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bc278-3338-47fc-9884-b471f0adb28c",
   "metadata": {},
   "source": [
    "### Initialize new modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a682d9-db2a-4c98-aae1-8753c8764572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import FeedForward, apply_rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "268b0336-0488-427e-8dd5-cb4a9291aa3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ddd663-c101-44f5-8890-3811688c9ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_local_heads = args.n_heads // 1\n",
    "        self.max_batch_size = args.max_batch_size\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "    \n",
    "        self.wq = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wk = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wv = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.cache_k = torch.zeros(\n",
    "            (self.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (self.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        )\n",
    "        if hiq.get_env_bool(\"KV_CAHCHE_IN_GPU\", True):\n",
    "            self.cache_k = self.cache_k.cuda()\n",
    "            self.cache_v = self.cache_v.cuda()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis:torch.Tensor):\n",
    "        start_pos = 262  \n",
    "        bsz, seqlen = self.max_batch_size, 1 \n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        #if mask is not None:\n",
    "            #scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)\n",
    "        output = output.transpose(\n",
    "            1, 2\n",
    "        ).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "809055bc-adb4-40fa-85da-bb69706e0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = Attention(model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfffcb2-6387-4b25-939d-d6ac41786db2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### load specific layer weight from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "656c0b78-4d9b-466c-b4a4-0a16e26bb6fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_id = 0\n",
    "\n",
    "name_list = []\n",
    "for name in ['wq', 'wk', 'wv', 'wo']:\n",
    "    full_layer_name = 'layers.' + str(layer_id) + '.attention.' + name + '.weight'\n",
    "    name_list.append(full_layer_name)\n",
    "    \n",
    "attention.wq.weight.data = checkpoint[name_list[0]]\n",
    "attention.wk.weight.data = checkpoint[name_list[1]]\n",
    "attention.wv.weight.data = checkpoint[name_list[2]]\n",
    "attention.wo.weight.data = checkpoint[name_list[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90905950-4fa8-43b7-9605-f2c227179839",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(dim=model_args.dim, hidden_dim=4 * model_args.dim, multiple_of=model_args.multiple_of).half()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eca77b-f52f-434f-9663-6e7eee9c95f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### load specific layer weight from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9313509d-0f49-4676-b673-bbbc9db3658c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name_list_ffn = []\n",
    "for name in ['w1', 'w2', 'w3']:\n",
    "    full_layer_name = 'layers.' + str(layer_id) + '.feed_forward.' + name + '.weight'\n",
    "    name_list.append(full_layer_name)\n",
    "\n",
    "ffn.w1.weight.data = checkpoint[name_list[-3]]\n",
    "ffn.w2.weight.data = checkpoint[name_list[-2]]\n",
    "ffn.w3.weight.data = checkpoint[name_list[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7da3b23b-0b1e-409f-8228-36085c939374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_norm = RMSNorm(model_args.dim, eps=model_args.norm_eps).half()\n",
    "ffn_norm = RMSNorm(model_args.dim, eps=model_args.norm_eps).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "188e60c3-9e82-4b25-9ff5-116d7b860b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_norm.weight.data = checkpoint['layers.0.attention_norm.weight']\n",
    "ffn_norm.weight.data = checkpoint['layers.0.ffn_norm.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1eb62b3-8a29-40d4-8470-1a5682c8f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = attention\n",
    "        self.feed_forward = ffn\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = attention_norm\n",
    "        self.ffn_norm = ffn_norm\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis:torch.Tensor):\n",
    "        #print(freqs_cis.shape)\n",
    "        h = x + self.attention.forward(self.attention_norm(x), freqs_cis)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4389d6ad-3eef-4e9d-93f5-ba6088e87ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_block = TransformerBlock(0, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a009faa-08dc-4663-b6f4-a6576db7fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'tf_block_static_weight_sq1_last_iter_vx.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ffdab31-9d37-4341-869a-037e5283c2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Workspace/LLaMA_IF/llama/model.py:87: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  xq_shape[-1] = int(xq_shape[-1]/2)\n",
      "/root/Workspace/LLaMA_IF/llama/model.py:90: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  xk_shape[-1] = int(xk_shape[-1]/2)\n",
      "/root/Workspace/LLaMA_IF/llama/model.py:58: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert freqs_cis.shape == (x.shape[1], x.shape[-2], 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.onnx.export(tf_block.half().cuda(), (h.cuda(), freqs_cis.cuda()), save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322c709-f6d3-488e-bccb-d3370e7b36cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81319c11-d659-49d6-a4c5-1a15342d53c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e268ee9-aa7f-4912-9efc-9cde0f1d7d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "attention.cache_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7167cf1-6ddb-4318-8ad1-4bd230b1dbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f095430-fb37-4bb9-a0ef-a8711af83420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536870912"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache = 128 * 512 * 32 *128 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddd5dfa4-0cc0-4b97-8fb6-bb8398edf517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202375168"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096 * 4096 * 4 +4096*11008 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a12bb84f-3150-4cef-9be9-a5a5a6a41f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739246080"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "202375168 + 536870912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4e05ec5-964a-4a7f-8ac0-ec675d6576c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.376953125"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "739246080 * 2 / 1024 /1024/ 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f1bf9-de75-4fe8-a49a-fd59a788488f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
