{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9002295-b5dc-45ca-8b26-9a3084c93778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import math\n",
    "import hiq\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from llama import ModelArgs, Transformer, Tokenizer, LLaMA\n",
    "from llama.generation import sample_top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f23a0a-3a3f-4a58-9199-523da991f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../7B/consolidated.00.pth', map_location=\"cpu\")\n",
    "\n",
    "with open('../7B/params.json', \"r\") as f:\n",
    "    params = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df823e3-b7d0-47bd-98f5-d1c5393e9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=512, max_batch_size=1, **params\n",
    ")\n",
    "tokenizer = Tokenizer('../tokenizer.model')\n",
    "model_args.vocab_size = tokenizer.n_words\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = Transformer(model_args)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "model.load_state_dict(checkpoint,strict= False)\n",
    "\n",
    "generator = LLaMA(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "161d2f5e-b4ac-46e1-917a-486544f084fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature: float = 0.8\n",
    "top_p: float = 0.95\n",
    "max_seq_len=512\n",
    "max_batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34d311df-209b-44ac-9c9d-47dd1b451666",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"I believe the meaning of life is\"]\n",
    "\n",
    "max_gen_len = 256\n",
    "\n",
    "bsz = 1 \n",
    "params = params\n",
    "prompt_tokens = [generator.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "total_len = min(max_seq_len, max_gen_len + max_prompt_size)\n",
    "\n",
    "tokens = torch.full((bsz, total_len),generator.tokenizer.pad_id).cuda().long()\n",
    "\n",
    "for k, t in enumerate(prompt_tokens):\n",
    "    tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "input_text_mask = tokens != generator.tokenizer.pad_id\n",
    "start_pos = min_prompt_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93938214-f6ec-4d50-bb2f-af99346e2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_pos = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for cur_pos in range(start_pos, total_len):\n",
    "        input_tensor = torch.cat((tokens[:, prev_pos:cur_pos],torch.tensor([[prev_pos]]).cuda()), 1)\n",
    "        if cur_pos == total_len-1:\n",
    "            break\n",
    "        logits = model(input_tensor)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "        next_token = next_token.reshape(-1)\n",
    "        # only replace token if prompt has already been generated\n",
    "        next_token = torch.where(\n",
    "            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "        )\n",
    "        tokens[:, cur_pos] = next_token\n",
    "        prev_pos = cur_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c03fe83-4ef1-4e25-ae41-4f076337dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.cat((tokens[:, prev_pos:cur_pos],torch.tensor([[prev_pos]]).cuda()), 1)\n",
    "\n",
    "token_len = input_tensor.shape[1] \n",
    "tokens = input_tensor[:, 0: token_len-1]\n",
    "start_pos = input_tensor[:, -1].item()\n",
    "_bsz, seqlen = tokens.shape\n",
    "h = model.tok_embeddings(tokens)\n",
    "model.freqs_cis = model.freqs_cis.to(h.device)\n",
    "freqs_cis = model.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "mask = None\n",
    "if seqlen > 1:\n",
    "    mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "    mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bc278-3338-47fc-9884-b471f0adb28c",
   "metadata": {},
   "source": [
    "### Initialize new modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3a682d9-db2a-4c98-aae1-8753c8764572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import FeedForward, apply_rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268b0336-0488-427e-8dd5-cb4a9291aa3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ddd663-c101-44f5-8890-3811688c9ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_local_heads = args.n_heads // 1\n",
    "        \n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "    \n",
    "        self.wq = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wk = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wv = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        )\n",
    "        if hiq.get_env_bool(\"KV_CAHCHE_IN_GPU\", True):\n",
    "            self.cache_k = self.cache_k.cuda()\n",
    "            self.cache_v = self.cache_v.cuda()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis:torch.Tensor):\n",
    "        start_pos = 262  \n",
    "        bsz, seqlen = 1, 1 \n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        #if mask is not None:\n",
    "            #scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)\n",
    "        output = output.transpose(\n",
    "            1, 2\n",
    "        ).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "809055bc-adb4-40fa-85da-bb69706e0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = Attention(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e2b4df-6320-4a77-afc8-bc6b3756f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load specific layer weight from checkpoint\n",
    "layer_id = 0\n",
    "\n",
    "name_list = []\n",
    "for name in ['wq', 'wk', 'wv', 'wo']:\n",
    "    full_layer_name = 'layers.' + str(layer_id) + '.attention.' + name + '.weight'\n",
    "    name_list.append(full_layer_name)\n",
    "    \n",
    "attention.wq.weight.data = checkpoint[name_list[0]]\n",
    "attention.wk.weight.data = checkpoint[name_list[1]]\n",
    "attention.wv.weight.data = checkpoint[name_list[2]]\n",
    "attention.wo.weight.data = checkpoint[name_list[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90905950-4fa8-43b7-9605-f2c227179839",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(dim=model_args.dim, hidden_dim=4 * model_args.dim, multiple_of=model_args.multiple_of).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21a6cf85-28f5-4179-a93b-e9f5f4d4fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load specific layer weight from checkpoint\n",
    "name_list_ffn = []\n",
    "for name in ['w1', 'w2', 'w3']:\n",
    "    full_layer_name = 'layers.' + str(layer_id) + '.feed_forward.' + name + '.weight'\n",
    "    name_list.append(full_layer_name)\n",
    "\n",
    "ffn.w1.weight.data = checkpoint[name_list[-3]]\n",
    "ffn.w2.weight.data = checkpoint[name_list[-2]]\n",
    "ffn.w3.weight.data = checkpoint[name_list[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec6ae900-c0ce-4211-9a18-b25731e9492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_norm = RMSNorm(model_args.dim, eps=model_args.norm_eps).half()\n",
    "ffn_norm = RMSNorm(model_args.dim, eps=model_args.norm_eps).half()\n",
    "\n",
    "attention_norm.weight.data = checkpoint['layers.0.attention_norm.weight']\n",
    "ffn_norm.weight.data = checkpoint['layers.0.ffn_norm.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1eb62b3-8a29-40d4-8470-1a5682c8f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = attention\n",
    "        self.feed_forward = ffn\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = attention_norm\n",
    "        self.ffn_norm = ffn_norm\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis:torch.Tensor):\n",
    "        #print(freqs_cis.shape)\n",
    "        h = x + self.attention.forward(self.attention_norm(x), freqs_cis)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4389d6ad-3eef-4e9d-93f5-ba6088e87ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_block = TransformerBlock(0, model_args).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a009faa-08dc-4663-b6f4-a6576db7fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'tf_block_1input_weight_sq1_last_iter_v4.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ffdab31-9d37-4341-869a-037e5283c2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Workspace/LLaMA_IF/llama/model.py:87: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  xq_shape[-1] = int(xq_shape[-1]/2)\n",
      "/root/Workspace/LLaMA_IF/llama/model.py:90: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  xk_shape[-1] = int(xk_shape[-1]/2)\n",
      "/root/Workspace/LLaMA_IF/llama/model.py:58: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert freqs_cis.shape == (x.shape[1], x.shape[-2], 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.onnx.export(tf_block, (h,freqs_cis), save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1aa0c5f-9f5f-4c54-9a65-2b832b57b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cbccf0b-3d20-406e-ac08-558092beb437",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf_block(h, freqs_cis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ad1fa8b-7d5d-4b6d-8a91-b66e988164c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.save('real_input_for_v4_tfblock', h.detach().cpu().numpy())\n",
    "numpy.save('real_output_for_v4_tfblock', output.detach().cpu().numpy())\n",
    "numpy.save('real_freq_for_v4_tfblock', freqs_cis.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f095430-fb37-4bb9-a0ef-a8711af83420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
