{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f18c1b-2918-4bea-bc95-46e524882685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import fire\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import hiq\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "#from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
    "\n",
    "from llama import ModelArgs, Transformer, Tokenizer, LLaMA\n",
    "from llama.generation import sample_top_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde4793-7e8b-44ec-a182-b81665d3c5ee",
   "metadata": {},
   "source": [
    "### load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c704bb-2fc9-41be-a5a7-0a58458aa41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../7B/consolidated.00.pth', map_location=\"cpu\")\n",
    "\n",
    "with open('../7B/params.json', \"r\") as f:\n",
    "    params = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aaf9a5-1b02-499f-94ca-93173e873297",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119a443f-b1d6-49f1-b815-9076068516e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=512, max_batch_size=1, **params\n",
    ")\n",
    "tokenizer = Tokenizer('../tokenizer.model')\n",
    "model_args.vocab_size = tokenizer.n_words\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = Transformer(model_args)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "model.load_state_dict(checkpoint,strict= False)\n",
    "\n",
    "generator = LLaMA(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19da2da0-c46f-4510-8cbb-6f9ae68dd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature: float = 0.8\n",
    "top_p: float = 0.95\n",
    "max_seq_len=512\n",
    "max_batch_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4503cca-77d2-4e55-9c74-bd6c6b29e939",
   "metadata": {},
   "source": [
    "### Generate internative input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002616aa-0018-4a30-a332-6012310fc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"I believe the meaning of life is\"]\n",
    "\n",
    "max_gen_len = 256\n",
    "\n",
    "bsz = 1 \n",
    "params = params\n",
    "prompt_tokens = [generator.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "total_len = min(max_seq_len, max_gen_len + max_prompt_size)\n",
    "\n",
    "tokens = torch.full((bsz, total_len),generator.tokenizer.pad_id).cuda().long()\n",
    "\n",
    "for k, t in enumerate(prompt_tokens):\n",
    "    tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "input_text_mask = tokens != generator.tokenizer.pad_id\n",
    "start_pos = min_prompt_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b50085-af9d-49e4-8eef-1add4888c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_pos = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for cur_pos in range(start_pos, total_len):\n",
    "        if cur_pos == total_len-1:\n",
    "            break\n",
    "        input_tensor = torch.cat((tokens[:, prev_pos:cur_pos],torch.tensor([[prev_pos]]).cuda()), 1)\n",
    "        logits = model(input_tensor)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "        next_token = next_token.reshape(-1)\n",
    "        # only replace token if prompt has already been generated\n",
    "        next_token = torch.where(\n",
    "            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "        )\n",
    "        tokens[:, cur_pos] = next_token\n",
    "        prev_pos = cur_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b11d0967-685d-42f1-b78b-7b37794a4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.cat((tokens[:, prev_pos:cur_pos],torch.tensor([[prev_pos]]).cuda()), 1)\n",
    "\n",
    "token_len = input_tensor.shape[1] \n",
    "tokens = input_tensor[:, 0: token_len-1]\n",
    "start_pos = input_tensor[:, -1].item()\n",
    "_bsz, seqlen = tokens.shape\n",
    "h = model.tok_embeddings(tokens)\n",
    "z = model.tok_embeddings(tokens)\n",
    "model.freqs_cis = model.freqs_cis.to(h.device)\n",
    "freqs_cis = model.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "mask = None\n",
    "if seqlen > 1:\n",
    "    mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "    mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec6c99-66e2-4f52-a987-7846bfc246f0",
   "metadata": {},
   "source": [
    "### Change the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e27c047-0d77-4d3e-a3c2-92e0c0dec5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import FeedForward, RMSNorm, apply_rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a35d423-0936-4df1-b6bb-8b619f87465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_local_heads = args.n_heads // 1\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "    \n",
    "        self.wq = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wk = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wv = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        )\n",
    "        if hiq.get_env_bool(\"KV_CAHCHE_IN_GPU\", True):\n",
    "            self.cache_k = self.cache_k.cuda()\n",
    "            self.cache_v = self.cache_v.cuda()\n",
    "\n",
    "    def forward(self, x: torch.Tensor,start_pos: torch.int64, freqs_cis:torch.Tensor):\n",
    "        bsz, seqlen = 1, 1 \n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)\n",
    "        output = output.transpose(\n",
    "            1, 2\n",
    "        ).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489b9d3-c43b-4e78-b93d-597cda550989",
   "metadata": {},
   "source": [
    "### Initialize the attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf753d2-ab55-499a-a230-97f33f6813e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = Attention(model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2aaa9a-ba94-49db-a895-e5a5b2b09a03",
   "metadata": {},
   "source": [
    "### Initialize the Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678a0023-01fc-4d79-993d-7997db9bf11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = attention\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: torch.int64, freqs_cis: torch.Tensor):\n",
    "        #print(freqs_cis.shape)\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "291b6364-e68e-4de6-bf6f-4515cb8e9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfb = TransformerBlock(0, model_args).half().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afda18-3e30-4f0a-b830-ca25e80c8cb7",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35744c98-fee6-4fcc-a0b5-5c01468a537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Workspace/LLaMA_IF/llama/model.py:87: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  xq_shape[-1] = int(xq_shape[-1]/2)\n",
      "/root/Workspace/LLaMA_IF/llama/model.py:90: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  xk_shape[-1] = int(xk_shape[-1]/2)\n",
      "/root/Workspace/LLaMA_IF/llama/model.py:58: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert freqs_cis.shape == (x.shape[1], x.shape[-2], 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.onnx.export(tfb, (h,start_pos, freqs_cis), 'transformer_block_sq1_last_iter_v2.onnx', opset_version=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6473e-28b0-4d43-80d3-968ee286cd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a3372-3041-47e7-b85d-4317896edc99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
