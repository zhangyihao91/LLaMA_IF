{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f18c1b-2918-4bea-bc95-46e524882685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import math\n",
    "import hiq\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from llama import ModelArgs, Transformer, Tokenizer, LLaMA\n",
    "from llama.generation import sample_top_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde4793-7e8b-44ec-a182-b81665d3c5ee",
   "metadata": {},
   "source": [
    "### load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c704bb-2fc9-41be-a5a7-0a58458aa41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../7B/consolidated.00.pth', map_location=\"cpu\")\n",
    "\n",
    "with open('../7B/params.json', \"r\") as f:\n",
    "    params = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aaf9a5-1b02-499f-94ca-93173e873297",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119a443f-b1d6-49f1-b815-9076068516e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=512, max_batch_size=1, **params\n",
    ")\n",
    "tokenizer = Tokenizer('../tokenizer.model')\n",
    "model_args.vocab_size = tokenizer.n_words\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = Transformer(model_args)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "model.load_state_dict(checkpoint,strict= False)\n",
    "\n",
    "generator = LLaMA(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19da2da0-c46f-4510-8cbb-6f9ae68dd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature: float = 0.8\n",
    "top_p: float = 0.95\n",
    "max_seq_len=512\n",
    "max_batch_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4503cca-77d2-4e55-9c74-bd6c6b29e939",
   "metadata": {},
   "source": [
    "### Generate internative input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002616aa-0018-4a30-a332-6012310fc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"I believe the meaning of life is\"]\n",
    "\n",
    "max_gen_len = 256\n",
    "\n",
    "bsz = 1 \n",
    "params = params\n",
    "prompt_tokens = [generator.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "total_len = min(max_seq_len, max_gen_len + max_prompt_size)\n",
    "\n",
    "tokens = torch.full((bsz, total_len),generator.tokenizer.pad_id).cuda().long()\n",
    "\n",
    "for k, t in enumerate(prompt_tokens):\n",
    "    tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "input_text_mask = tokens != generator.tokenizer.pad_id\n",
    "start_pos = min_prompt_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b50085-af9d-49e4-8eef-1add4888c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_pos = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for cur_pos in range(start_pos, total_len):\n",
    "        if cur_pos == total_len-1:\n",
    "            break\n",
    "        input_tensor = torch.cat((tokens[:, prev_pos:cur_pos],torch.tensor([[prev_pos]]).cuda()), 1)\n",
    "        logits = model(input_tensor)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "        next_token = next_token.reshape(-1)\n",
    "        # only replace token if prompt has already been generated\n",
    "        next_token = torch.where(\n",
    "            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "        )\n",
    "        tokens[:, cur_pos] = next_token\n",
    "        prev_pos = cur_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b11d0967-685d-42f1-b78b-7b37794a4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.cat((tokens[:, prev_pos:cur_pos],torch.tensor([[prev_pos]]).cuda()), 1)\n",
    "\n",
    "token_len = input_tensor.shape[1] \n",
    "tokens = input_tensor[:, 0: token_len-1]\n",
    "start_pos = input_tensor[:, -1].item()\n",
    "_bsz, seqlen = tokens.shape\n",
    "h = model.tok_embeddings(tokens)\n",
    "z = model.tok_embeddings(tokens)\n",
    "model.freqs_cis = model.freqs_cis.to(h.device)\n",
    "freqs_cis = model.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "mask = None\n",
    "if seqlen > 1:\n",
    "    mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "    mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec6c99-66e2-4f52-a987-7846bfc246f0",
   "metadata": {},
   "source": [
    "### Change the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e27c047-0d77-4d3e-a3c2-92e0c0dec5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.model import FeedForward, RMSNorm, apply_rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a35d423-0936-4df1-b6bb-8b619f87465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_local_heads = args.n_heads // 1\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "    \n",
    "        self.wq = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wk = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wv = nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        )\n",
    "        if hiq.get_env_bool(\"KV_CAHCHE_IN_GPU\", True):\n",
    "            self.cache_k = self.cache_k.cuda()\n",
    "            self.cache_v = self.cache_v.cuda()\n",
    "\n",
    "    def forward(self, x: torch.Tensor,start_pos: torch.int64, freqs_cis:torch.Tensor):\n",
    "        bsz, seqlen = 1, 1 \n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)\n",
    "        output = output.transpose(\n",
    "            1, 2\n",
    "        ).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489b9d3-c43b-4e78-b93d-597cda550989",
   "metadata": {},
   "source": [
    "### Initialize the attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf753d2-ab55-499a-a230-97f33f6813e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = Attention(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "465f87a9-43bd-4e33-8be5-c867274f99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load specific layer weight from checkpoint\n",
    "layer_id = 0\n",
    "\n",
    "name_list = []\n",
    "for name in ['wq', 'wk', 'wv', 'wo']:\n",
    "    full_layer_name = 'layers.' + str(layer_id) + '.attention.' + name + '.weight'\n",
    "    name_list.append(full_layer_name)\n",
    "    \n",
    "attention.wq.weight.data = checkpoint[name_list[0]]\n",
    "attention.wk.weight.data = checkpoint[name_list[1]]\n",
    "attention.wv.weight.data = checkpoint[name_list[2]]\n",
    "attention.wo.weight.data = checkpoint[name_list[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063df88-106a-43b8-8d47-15867f503888",
   "metadata": {},
   "source": [
    "### Initialize the ffn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90d9eb09-feb0-49f5-ada3-ed4da01ab466",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(dim=model_args.dim, hidden_dim=4 * model_args.dim, multiple_of=model_args.multiple_of).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fffaa7de-59ff-4c9d-96dc-3ae2f3cc70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load specific layer weight from checkpoint\n",
    "name_list_ffn = []\n",
    "for name in ['w1', 'w2', 'w3']:\n",
    "    full_layer_name = 'layers.' + str(layer_id) + '.feed_forward.' + name + '.weight'\n",
    "    name_list.append(full_layer_name)\n",
    "\n",
    "ffn.w1.weight.data = checkpoint[name_list[0]]\n",
    "ffn.w2.weight.data = checkpoint[name_list[1]]\n",
    "ffn.w3.weight.data = checkpoint[name_list[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f8ec93-7f73-457c-865b-09540fa00148",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize the layer_norm module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63e5954b-294f-4c9c-8755-3e6ea308a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_norm = RMSNorm(model_args.dim, eps=model_args.norm_eps).half()\n",
    "ffn_norm = RMSNorm(model_args.dim, eps=model_args.norm_eps).half()\n",
    "\n",
    "attention_norm.weight.data = checkpoint['layers.0.attention_norm.weight']\n",
    "ffn_norm.weight.data = checkpoint['layers.0.ffn_norm.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2aaa9a-ba94-49db-a895-e5a5b2b09a03",
   "metadata": {},
   "source": [
    "### Initialize the Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "678a0023-01fc-4d79-993d-7997db9bf11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = attention\n",
    "        self.feed_forward = ffn\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = attention_norm\n",
    "        self.ffn_norm = ffn_norm\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: torch.int64, freqs_cis: torch.Tensor):\n",
    "        #print(freqs_cis.shape)\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "291b6364-e68e-4de6-bf6f-4515cb8e9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfb = TransformerBlock(0, model_args).half().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afda18-3e30-4f0a-b830-ca25e80c8cb7",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35744c98-fee6-4fcc-a0b5-5c01468a537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Workspace/LLaMA_IF/llama/model.py:87: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  xq_shape[-1] = int(xq_shape[-1]/2)\n",
      "/root/Workspace/LLaMA_IF/llama/model.py:90: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  xk_shape[-1] = int(xk_shape[-1]/2)\n",
      "/root/Workspace/LLaMA_IF/llama/model.py:58: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert freqs_cis.shape == (x.shape[1], x.shape[-2], 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.onnx.export(tfb, (h,start_pos, freqs_cis), 'transformer_block_sq1_last_iter_v2.5.onnx', opset_version=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6473e-28b0-4d43-80d3-968ee286cd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a3372-3041-47e7-b85d-4317896edc99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
