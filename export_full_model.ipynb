{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00889468-6214-4b07-a002-ad1f41098d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import fire\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
    "\n",
    "from llama import ModelArgs, Transformer, Tokenizer, LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3346a3ba-35e4-4e0e-9ca2-572bf44831a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.generation import sample_top_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff3e9ed-2708-4e51-8c05-a8d398d1935b",
   "metadata": {},
   "source": [
    "### Load Model/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9203ea-1d58-451e-af1d-2e3d129f3ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../7B/consolidated.00.pth', map_location=\"cpu\")\n",
    "\n",
    "with open('../7B/params.json', \"r\") as f:\n",
    "    params = json.loads(f.read())\n",
    "\n",
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=512, max_batch_size=1, **params\n",
    ")\n",
    "tokenizer = Tokenizer('../tokenizer.model')\n",
    "model_args.vocab_size = tokenizer.n_words\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = Transformer(model_args)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "model.load_state_dict(checkpoint,strict= False)\n",
    "\n",
    "generator = LLaMA(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e6315-2f57-4832-a1e6-cb7b4847fb1f",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f16142b-1f27-498d-919e-7500f3e6b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature: float = 0.8\n",
    "top_p: float = 0.95\n",
    "max_seq_len=512\n",
    "max_batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccd0b71-a9f8-4c5c-ba7a-3e03c0359e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['I believe the meaning of life is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dece3b48-9d22-49f5-8b1c-f6a7262dd51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_gen_len = 256\n",
    "\n",
    "bsz = 1 \n",
    "params = params\n",
    "prompt_tokens = [tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "total_len = min(max_seq_len, max_gen_len + max_prompt_size)\n",
    "\n",
    "tokens = torch.full((bsz, total_len),tokenizer.pad_id).cuda().long()\n",
    "\n",
    "for k, t in enumerate(prompt_tokens):\n",
    "    tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "input_text_mask = tokens != generator.tokenizer.pad_id\n",
    "start_pos = min_prompt_size\n",
    "prev_pos = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec8577-7919-44cb-983c-e38e3e9678a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02cb3913-6401-4efb-af56-18507c352df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur_pos = start_pos\n",
    "\n",
    "dummy_input = torch.cat((tokens[:, prev_pos:cur_pos],torch.tensor([[prev_pos]]).cuda()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "767ec416-566d-4b81-b988-afc79da55d0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Workspace/nopara_llama/llama/model.py:278: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  start_pos = input_tensor[:, -1].item()\n",
      "/root/Workspace/nopara_llama/llama/model.py:87: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  xq_shape[-1] = int(xq_shape[-1]/2)\n",
      "/root/Workspace/nopara_llama/llama/model.py:90: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  xk_shape[-1] = int(xk_shape[-1]/2)\n",
      "/root/Workspace/nopara_llama/llama/model.py:58: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert freqs_cis.shape == (x.shape[1], x.shape[-2], 2)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    trace = torch.jit.trace(generator.model, dummy_input)\n",
    "    torch.jit.save(trace, './full_llama.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c590f05-d309-4af4-ab4f-64071af19dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i, t in enumerate(tokens.tolist()):\n",
    "    # cut to max gen len\n",
    "    t = t[: len(prompt_tokens[i]) + max_gen_len]\n",
    "    # cut to eos tok if any\n",
    "    try:\n",
    "        t = t[: t.index(generator.tokenizer.eos_id)]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    decoded.append(generator.tokenizer.decode(t))\n",
    "\n",
    "decoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
